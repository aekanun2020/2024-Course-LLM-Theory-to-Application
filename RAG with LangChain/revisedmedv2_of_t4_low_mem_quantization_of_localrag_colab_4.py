# -*- coding: utf-8 -*-
"""revisedMedv2_of_T4_low_mem_Quantization_of_localrag_colab_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XRI61l2GHx5AoADsZrw4uRT-aJAJ7u1b

T4 on Colab
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %pip install accelerate peft bitsandbytes transformers trl

! pip install langchain_community

! pip install sentence-transformers

!pip install pypdf tiktoken langchain sentence-transformers

!pip install faiss-cpu

question_text = "ผมมีไข้และมีผื่นขึ้นตามตัว ผมเป็นโรคอะไร?"

device = 'cpu'

"""# 1. Loading and Splitting Documents (PDF)"""

! mkdir -p ./corpus_input

! wget -O ./corpus_input/1.pdf https://storage.googleapis.com/corpus_input/1.pdf
! wget -O ./corpus_input/2.pdf https://storage.googleapis.com/corpus_input/2.pdf
! wget -O ./corpus_input/3.pdf https://storage.googleapis.com/corpus_input/3.pdf
! wget -O ./corpus_input/44.pdf https://storage.googleapis.com/corpus_input/44.pdf
! wget -O ./corpus_input/5555.pdf https://storage.googleapis.com/corpus_input/5555.pdf

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# load PDFs
loaders = [
    #PyPDFLoader("/home/jupyter/corpus_input/pobpad-1.pdf"),
    PyPDFLoader("./corpus_input/1.pdf"),
    PyPDFLoader("./corpus_input/2.pdf"),
    PyPDFLoader("./corpus_input/3.pdf"),
    PyPDFLoader("./corpus_input/44.pdf"),
    PyPDFLoader("./corpus_input/5555.pdf"),

]
pages = []
for loader in loaders:
    pages.extend(loader.load())

# split text to chunks
text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
    tokenizer=AutoTokenizer.from_pretrained(
        "sentence-transformers/all-MiniLM-L12-v2"
     ),
     chunk_size=256,
     chunk_overlap=32,
     strip_whitespace=True,
)

docs = text_splitter.split_documents(pages)

"""# 2. Embedding and Creating Vector Database"""

from langchain_community.embeddings import (
    HuggingFaceEmbeddings
)

encoder = HuggingFaceEmbeddings(
    model_name = 'sentence-transformers/all-MiniLM-L12-v2',
    model_kwargs = {'device': device}
)

from langchain.vectorstores import FAISS
from langchain_community.vectorstores.utils import DistanceStrategy

faiss_db = FAISS.from_documents(
    docs, encoder, distance_strategy=DistanceStrategy.COSINE
)

"""# 3. Search and Document Retrieval"""

retrieved_docs = faiss_db.similarity_search(question_text, k=1)

retrieved_docs

"""# 4. Text Response Generators"""

import torch
from accelerate import Accelerator

accelerator = Accelerator()

tokenizer = AutoTokenizer.from_pretrained("Aekanun/openthaigpt-MedChatModelv2")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    "Aekanun/openthaigpt-MedChatModelv2",
    quantization_config=bnb_config,
    trust_remote_code=True
)
model.config.use_cache = False

def generate(question: str, context: str):
    if context == None or context == "":
        prompt = f"""<s>[INST] <<SYS>You are a question answering assistant. Answer the question as truthful and helpful as possible คุณคือผู้ช่วยตอบคำถาม จงตอบคำถามอย่างถูกต้องและมีประโยชน์ที่สุด<</SYS>>{question}[/INST]"""
    else:
        prompt = f"""<s>[INST] <<SYS>You are a question answering assistant. Answer the question as truthful and helpful as possible คุณคือผู้ช่วยตอบคำถาม จงตอบคำถามอย่างถูกต้องและมีประโยชน์ที่สุด<</SYS>>
            Context: {context}.
            Question: {question}[/INST]"""
    chat = [{"role": "user", "content": prompt}]

    print(chat)
    formatted_prompt = tokenizer.apply_chat_template(
        chat,
        tokenize=False,
        add_generation_prompt=True,
    )
    inputs = tokenizer.encode(
        formatted_prompt, add_special_tokens=False, return_tensors="pt"
    ).to(device)
    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs,
            max_new_tokens=250,
            do_sample=False,
        )
    response = tokenizer.decode(outputs[0], skip_special_tokens=False)
    response = response[len(formatted_prompt) :]  # remove input prompt from reponse
    response = response.replace("<eos>", "")  # remove eos token
    return response

question_text

print(generate(question=question_text, context=retrieved_docs[0].page_content))

retrieved_docs