# -*- coding: utf-8 -*-
"""localrag_colab-4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19iKyCDDcxACixer1ep0ItSMH1M-GlRqX

CPU + RAM 60-80 GB (เลือกใช้ GPU: A100 ของ Colab Pro+)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %pip install accelerate peft bitsandbytes transformers trl

! pip install langchain_community

! pip install sentence-transformers

!pip install pypdf tiktoken langchain sentence-transformers

!pip install faiss-cpu

question_text = "ผมมีไข้และมีผื่นขึ้นตามตัว ผมเป็นโรคอะไร?"

device = 'cpu'

"""# 1. Text Response Generators"""

import torch
from accelerate import Accelerator

accelerator = Accelerator()

from transformers import AutoModelForCausalLM, AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("Aekanun/openthaigpt-MedChatModel")
model = AutoModelForCausalLM.from_pretrained("Aekanun/openthaigpt-MedChatModel")

def generate(question: str, context: str):
    if context == None or context == "":
        prompt = f"""Give a detailed answer to the following question. Question: {question}"""
    else:
        prompt = f"""Using the information contained in the context, give a detailed answer to the question.
            Context: {context}.
            Question: {question}"""
    chat = [{"role": "user", "content": prompt}]
    formatted_prompt = tokenizer.apply_chat_template(
        chat,
        tokenize=False,
        add_generation_prompt=True,
    )
    inputs = tokenizer.encode(
        formatted_prompt, add_special_tokens=False, return_tensors="pt"
    ).to(device)
    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs,
            max_new_tokens=250,
            do_sample=False,
        )
    response = tokenizer.decode(outputs[0], skip_special_tokens=False)
    response = response[len(formatted_prompt) :]  # remove input prompt from reponse
    response = response.replace("<eos>", "")  # remove eos token
    return response

print(generate(question=question_text, context=""))

"""# 2. Splitting Documents (PDF)"""

! mkdir -p ./corpus_input

! wget -O ./corpus_input/1.pdf https://storage.googleapis.com/corpus_input/1.pdf
! wget -O ./corpus_input/2.pdf https://storage.googleapis.com/corpus_input/2.pdf
! wget -O ./corpus_input/3.pdf https://storage.googleapis.com/corpus_input/3.pdf
! wget -O ./corpus_input/44.pdf https://storage.googleapis.com/corpus_input/44.pdf
! wget -O ./corpus_input/5555.pdf https://storage.googleapis.com/corpus_input/5555.pdf

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# load PDFs
loaders = [
    #PyPDFLoader("/home/jupyter/corpus_input/pobpad-1.pdf"),
    PyPDFLoader("./corpus_input/1.pdf"),
    PyPDFLoader("./corpus_input/2.pdf"),
    PyPDFLoader("./corpus_input/3.pdf"),
    PyPDFLoader("./corpus_input/44.pdf"),
    PyPDFLoader("./corpus_input/5555.pdf"),

]
pages = []
for loader in loaders:
    pages.extend(loader.load())

# split text to chunks
text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
    tokenizer=AutoTokenizer.from_pretrained(
        "sentence-transformers/all-MiniLM-L12-v2"
     ),
     chunk_size=256,
     chunk_overlap=32,
     strip_whitespace=True,
)

docs = text_splitter.split_documents(pages)

"""# 3. Embedding and Creating Vector Database"""

from langchain_community.embeddings import (
    HuggingFaceEmbeddings
)

encoder = HuggingFaceEmbeddings(
    model_name = 'sentence-transformers/all-MiniLM-L12-v2',
    model_kwargs = {'device': device}
)

from langchain.vectorstores import FAISS
from langchain_community.vectorstores.utils import DistanceStrategy

faiss_db = FAISS.from_documents(
    docs, encoder, distance_strategy=DistanceStrategy.COSINE
)

"""# 4. Search and Document Retrieval"""

retrieved_docs = faiss_db.similarity_search(question_text, k=1)

retrieved_docs

faiss_db

