# -*- coding: utf-8 -*-
"""revisionPrompt-and-HyperParam_of_OpenThaiGPT_FineTuning_withMedKnowledge_with100percentrows.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MOohmD8wzhGUxI8XXt2EUn68eK1_lWd1
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %pip install accelerate peft bitsandbytes transformers trl

!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git
!pip install -q datasets bitsandbytes einops wandb

from datasets import load_dataset

med_dataset = "Thaweewat/thai-med-pack"

huggingface_dataset = load_dataset(med_dataset, split="train")

huggingface_dataset

import pandas as pd

# การสุ่มตัวอย่าง 10% ของข้อมูลโดยไม่ให้มีการหยิบซ้ำ
dataset_pd = pd.DataFrame(huggingface_dataset).sample(frac=0.1, replace=False)

dataset_pd.info()

from datasets import Dataset

# สมมุติว่า df เป็น DataFrame ที่คุณได้แก้ไขแล้ว
customized_dataset = Dataset.from_pandas(dataset_pd)

def transform_structure(example):
    # Extract and split the text
    instruction_text = example['text'].split('[INST] ')[1].split(' [/INST]')[0]
    response_text = example['text'].split('[INST] ')[1].split(' [/INST]')[1].strip()

    # Strip off the ending token </s> if it's present
    if response_text.endswith('</s>'):
        response_text = response_text[:-4].strip()

    # Return the new structure
    return {
        'instruction': instruction_text,
        'input': '',
        'output': response_text,
        'text': ''
    }

# Apply the transformation to each record in the dataset again
transformed_dataset = customized_dataset.map(transform_structure)

customized_dataset[18000]

transformed_dataset[18000]

from google.colab import drive
drive.mount('/content/drive')

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer

model_name = "openthaigpt/openthaigpt-1.0.0-beta-13b-chat-hf"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    trust_remote_code=True
)
model.config.use_cache = False

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

from peft import LoraConfig

lora_alpha = 16
lora_dropout = 0.1
lora_r = 64

peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=[
        "self_attn.q_proj",
        "self_attn.k_proj",
        "self_attn.v_proj",
        "self_attn.o_proj",
        # Add other relevant submodules from LlamaDecoderLayer here
    ]
)

# ตัวอย่างการกำหนดขนาดข้อมูลการฝึก
per_device_train_batch_size = 2 ## จากเดิม 4
gradient_accumulation_steps = 4 ## จากเดิม 2
total_train_data_size = len(transformed_dataset)  # จำนวนข้อมูลฝึกทั้งหมด
batch_size = per_device_train_batch_size * 1  # ถ้ามี 1 GPU
steps_per_epoch = int(total_train_data_size / (batch_size * gradient_accumulation_steps))
desired_epochs = 1  # ตัวอย่างจำนวน epoch ที่ต้องการ

# คำนวณ max_steps ที่จะตั้งใน TrainingArguments
max_steps_calculated = steps_per_epoch * desired_epochs

max_steps_calculated

from transformers import TrainingArguments

output_dir = "/content/drive/MyDrive/llm_ThaiMed_checkpoints/thai-med-pack"
gradient_accumulation_steps = gradient_accumulation_steps
optim = "paged_adamw_32bit"
save_steps = 50
logging_steps = 10
learning_rate = 2e-4
max_grad_norm = 0.3
max_steps = 100
warmup_ratio = 0.03
lr_scheduler_type = "cosine"

training_arguments = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    fp16=True,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps_calculated,
    warmup_ratio=warmup_ratio,
    group_by_length=True,
    lr_scheduler_type=lr_scheduler_type,
)

def formatting_prompts_func(examples):
    output_text = []
    for i in range(len(examples)):
        instruction = examples["instruction"][i]
        response = examples["output"][i]
        text = f'<s>[INST] <<SYS>You are a question answering assistant. Answer the question as truthful and helpful as possible คุณคือผู้ช่วยตอบคำถาม จงตอบคำถามอย่างถูกต้องและมีประโยชน์ที่สุด<</SYS>>{instruction}[/INST]{response}'
        output_text.append(text)
    return output_text

model

from trl import SFTTrainer

max_seq_length = 512

trainer = SFTTrainer(
    model=model,
    train_dataset=transformed_dataset,
    peft_config=peft_config,
    formatting_func=formatting_prompts_func,
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=training_arguments,
)

for name, module in trainer.model.named_modules():
    if "norm" in name:
        module = module.to(torch.float32)

trainer.train()

"""### Model Inference"""

!nvidia-smi

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer

model_name = "openthaigpt/openthaigpt-1.0.0-beta-13b-chat-hf"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    trust_remote_code=True
)
model.config.use_cache = False

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

custom_prompt = "โปรดอธิบายลักษณะช่องปากที่เป็นมะเร็งในระยะเริ่มต้น"

PROMPT =f'<s>[INST] <<SYS>You are a question answering assistant. Answer the question as truthful and helpful as possible คุณคือผู้ช่วยตอบคำถาม จงตอบคำถามอย่างถูกต้องและมีประโยชน์ที่สุด<</SYS>>{custom_prompt}[/INST]'

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from peft import PeftModel
# # from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig
# from transformers import GenerationConfig
# 
# model = PeftModel.from_pretrained(model, "/content/drive/MyDrive/llm_ThaiMed_checkpoints/thai-med-pack/checkpoint-100")
# 
# inputs = tokenizer(
#     PROMPT,
#     return_tensors="pt",
# )
# input_ids = inputs["input_ids"].cuda()
# 
# generation_config = GenerationConfig(
#     temperature=0.6,
#     top_p=0.95,
#     repetition_penalty=1.15,
# )
# print("Generating...")
# generation_output = model.generate(
#     input_ids=input_ids,
#     generation_config=generation_config,
#     return_dict_in_generate=True,
#     output_scores=True,
#     max_new_tokens=256,
#     pad_token_id=tokenizer.eos_token_id
# )
# for s in generation_output.sequences:
#     print(tokenizer.decode(s))

import torch
torch.cuda.empty_cache()

custom_prompt = "ถูกงูพิษกัดต้องทำอย่างไรครับ"
PROMPT =f'<s>[INST] <<SYS>You are a question answering assistant. Answer the question as truthful and helpful as possible คุณคือผู้ช่วยตอบคำถาม จงตอบคำถามอย่างถูกต้องและมีประโยชน์ที่สุด<</SYS>>{custom_prompt}[/INST]'

inputs = tokenizer(
    PROMPT,
    return_tensors="pt",
)
input_ids = inputs["input_ids"].cuda()
print("Generating...")
generation_output = model.generate(
    input_ids=input_ids,
    generation_config=generation_config,
    return_dict_in_generate=True,
    output_scores=True,
    max_new_tokens=256,
    pad_token_id=tokenizer.eos_token_id
)
for s in generation_output.sequences:
    print(tokenizer.decode(s))

model

model.save_pretrained('finetunedMedmodelv2')
tokenizer.save_pretrained('finetunedMedmodelv2')

model.push_to_hub('openthaigpt-MedChatModelv2')
tokenizer.push_to_hub('openthaigpt-MedChatModelv2')

