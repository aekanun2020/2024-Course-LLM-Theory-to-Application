{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a45062df-18b0-4a3d-b570-b67233d36be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlpaug\n",
      "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from nlpaug) (1.24.1)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from nlpaug) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.22.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from nlpaug) (2.31.0)\n",
      "Collecting gdown>=4.0.0 (from nlpaug)\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (3.11.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug) (4.66.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from pandas>=1.2.0->nlpaug) (2022.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (2023.11.17)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->nlpaug) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
      "Installing collected packages: gdown, nlpaug\n",
      "Successfully installed gdown-5.2.0 nlpaug-1.1.11\n"
     ]
    }
   ],
   "source": [
    "! pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfeacb13-b09a-4e92-a1bc-0d8271515f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <8E3FD81A-C2E9-3A49-B4B9-6094D47528A4> /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <EACD001F-FCB9-380E-AD73-D522177FC040> /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/grizzlystudio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/grizzlystudio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text: the quick brown fox jumps over the lazy dog ai is transforming various industries\n",
      "Tokens: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', 'ai', 'transforming', 'various', 'industries']\n",
      "Augmented text: ['The prompt brownness fox jumps over the lazy hotdog. three toed sloth is transforming diverse industries.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da62dafe8f64447d8aa48cd4ff01bb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97fa24f1434247848ec397f5f886d3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9070aed080804a618854c19ea00cb2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14afe466a8f04a819778038c37f5cc3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM tokens: [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899] ...\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 1. Text Cleaning\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# 2. Tokenization\n",
    "def tokenize_text(text):\n",
    "    # Tokenize using NLTK\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# 3. Data Augmentation\n",
    "def augment_text(text):\n",
    "    # Initialize augmenter\n",
    "    aug = naw.SynonymAug(aug_src='wordnet')\n",
    "    # Augment text\n",
    "    augmented_text = aug.augment(text)\n",
    "    return augmented_text\n",
    "\n",
    "# 4. Tokenization for LLM (using BERT tokenizer as an example)\n",
    "def tokenize_for_llm(text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    return encoded\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog. AI is transforming various industries.\"\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean_text(sample_text)\n",
    "print(\"Cleaned text:\", cleaned_text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenize_text(cleaned_text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Augment the text\n",
    "augmented_text = augment_text(sample_text)\n",
    "print(\"Augmented text:\", augmented_text)\n",
    "\n",
    "# Tokenize for LLM\n",
    "llm_tokens = tokenize_for_llm(cleaned_text)\n",
    "print(\"LLM tokens:\", llm_tokens['input_ids'][:10], \"...\")  # Showing first 10 tokens\n",
    "print(\"Attention mask:\", llm_tokens['attention_mask'][:10], \"...\")  # Showing first 10 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15e1de1d-fcf9-46e8-ba37-2f16fcc376bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: 101, Word: [CLS]\n",
      "Token ID: 1996, Word: the\n",
      "Token ID: 4248, Word: quick\n",
      "Token ID: 2829, Word: brown\n",
      "Token ID: 4419, Word: fox\n",
      "Token ID: 14523, Word: jumps\n",
      "Token ID: 2058, Word: over\n",
      "Token ID: 1996, Word: the\n",
      "Token ID: 13971, Word: lazy\n",
      "Token ID: 3899, Word: dog\n",
      "\n",
      "Decoded text: [CLS] the quick brown fox jumps over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# The token IDs we got from the previous output\n",
    "token_ids = [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899]\n",
    "\n",
    "# Decode the tokens back to words\n",
    "words = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "# Print each token ID and its corresponding word\n",
    "for token_id, word in zip(token_ids, words):\n",
    "    print(f\"Token ID: {token_id}, Word: {word}\")\n",
    "\n",
    "# Decode the entire sequence back to text\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"\\nDecoded text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62fff5-56eb-4af6-bba9-e51a03f3a988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
